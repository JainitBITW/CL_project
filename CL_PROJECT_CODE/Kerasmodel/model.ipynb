{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 12:30:22.508960: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-14 12:30:22.736674: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdbEr.txt', 'README', 'train', 'imdb.vocab', 'test']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset ='/home/turning/Desktop/CL_project/CL_PROJECT_CODE/Kerasmodel/aclImdb'\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = '/home/turning/Desktop/CL_project/CL_PROJECT_CODE/Teamwise_dataset'\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/turning/Desktop/CL_project/CL_PROJECT_CODE/Teamwise_dataset/unsup'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/turning/Desktop/CL_project/CL_PROJECT_CODE/Kerasmodel/model.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/turning/Desktop/CL_project/CL_PROJECT_CODE/Kerasmodel/model.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m remove_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(train_dir, \u001b[39m'\u001b[39m\u001b[39munsup\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/turning/Desktop/CL_project/CL_PROJECT_CODE/Kerasmodel/model.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m shutil\u001b[39m.\u001b[39;49mrmtree(remove_dir)\n",
      "File \u001b[0;32m~/anaconda3/envs/neural_n/lib/python3.10/shutil.py:714\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    712\u001b[0m     orig_st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlstat(path)\n\u001b[1;32m    713\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m--> 714\u001b[0m     onerror(os\u001b[39m.\u001b[39;49mlstat, path, sys\u001b[39m.\u001b[39;49mexc_info())\n\u001b[1;32m    715\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/neural_n/lib/python3.10/shutil.py:712\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[39m# Note: To guard against symlink races, we use the standard\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39m# lstat()/open()/fstat() trick.\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 712\u001b[0m     orig_st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlstat(path)\n\u001b[1;32m    713\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     onerror(os\u001b[39m.\u001b[39mlstat, path, sys\u001b[39m.\u001b[39mexc_info())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/turning/Desktop/CL_project/CL_PROJECT_CODE/Teamwise_dataset/unsup'"
     ]
    }
   ],
   "source": [
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21668 files belonging to 2 classes.\n",
      "Using 17335 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 12:30:31.580361: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21668 files belonging to 2 classes.\n",
      "Using 4333 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 123\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='training', seed=seed)\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='validation', seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 b'Okay. Who was it? Who gave Revolver 10 out of 10? Are you tripping of your head on Ecstasy pipes? There were so many of you. Did you do it for a dare? Is this some kind of cult? Or did Guy Richie himself sign up 788 times under different names?<br /><br />Before I say anything else, I\\'ll say this. Just because you don\\'t understand a film doesn\\'t mean that it\\'s not great. Maybe you\\'ve had a bad day at work, or you sat down to watch a film after you had a row with your wife and then weren\\'t in the mood. Maybe there\\'s a more fundamental stumbling block- like you just don\\'t have the mental capacity or a highly enough developed philosophical sense to engage with it. BUT. And this is a very, very big but. The XXL elephant-sized mega-but to end all buts.<br /><br />PLEASE don\\'t confuse incoherence for complexity, and please don\\'t confuse this two hour non-squirter for an interesting film. Really. You may think you are pretty smart. You may even think of yourself as somewhat of a romantic figure: an independent thinker championing a masterpiece against a chorus of sheep-like naysayers. Please don\\'t. You\\'re embarrassing yourself. <br /><br />Revolver\\'s a waste of everyone\\'s time. If you thought about if for a few minutes, you\\'d recognise it too. It was a waste of the cast, a waste of the crew, a waste of the caterers, and definitely a waste of the precious minutes (you can\\'t get them back you know) of anyone unlucky enough to sit through this unutterable, wretched mess.<br /><br />\"No - wait,\" comes a voice in the darkness. \"You just don\\'t understand. Its NON-LINEAR. That means the story doesn\\'t go in a STRAIGHT LINE. This is actually the COMPLEX and SUBTLE work of an AUTEUR. It addresses difficult EXISTENTIAL questions. And anyway - they slated FIGHT CLUB when it first came out - didn\\'t you hear? -Because they couldn\\'t deal with the COMPLEXITY. They\\'re eating humble pie now. Bet you hate Lynch films too, doncha?\" <br /><br />Hate to disappoint you, but I am quite a big Lynch fan. I rather like Memento, so a narrative told in an unconventional fashion doesn\\'t necessarily fill me with fear. And although I\\'ve only studied it briefly a few years ago, philosophy interests me greatly. I don\\'t dislike Revolver for these reasons. I dislike it because it purports to be about weighty, big-brained topics but deals with them in such an insultingly superficial way as to be laughable. I\\'m not much of a chess player, but Richie\\'s idea of how chess works seems to be that of a precocious four year old. I dislike it because the characters, without exception, totally alienated me. \"Aha!\" cries the Richie apologist. \"Guy is cleverly tipping his hat to Brecht!\" Just maybe you\\'re right. I think its more likely that he just can\\'t write a decent script for toffee.<br /><br />Comparing Revolver with Fight Club is actually really instructive. Fight Club has acid-tongued, nihilistic dialogue that makes you laugh. Revolver has stale fortune cookie reject one-liners that make your ears bleed. Fight Club has a great twist that makes you reassess everything that has happened. Revolver has, as far as I can tell, several incomprehensible twists that offer no satisfaction because... well, they don\\'t make sense. If you keep pulling the rug out from under people, they eventually kick you out of their house. And then they lock all the doors and windows. And they never let you back in. Ever.<br /><br />Guy Richie seems to assume that being philosophical entails repeating a mantra of little buzz-phrases. Mostly they are spoken, but often they flash up on the screen with attributions. It\\'s almost pathological.<br /><br />But what makes this film particularly notable is the way in which something so incomprehensible can be married so neatly with all tired gangster clich\\xc3\\xa9s in the world. Ultimately its so inconsequential. You don\\'t care about anything. You don\\'t understand anything. You go home.<br /><br />Actually, there was a bit I really liked: the uptight assassin who has a crisis of confidence. He\\'s great. But I can\\'t recommend you see the film just to see him. He\\'s only in it for a few minutes.<br /><br />Please believe me. It\\'s horrible.'\n",
      "1 b'What a great gem this is. It\\'s an unusual story that is fun to watch. Yes, it has singing, but it is very nicely crafted into the story and is very melodic to hear. It was so pleasant to watch; I enjoyed it from start to finish! <br /><br />The movie takes place in England during World War II. It is about an apprentice witch who is searching for a missing portion of a spell that she needs. She uses her rough \"magic\" to transport her and 3 children under her care to various destinations to find it. They are joined by her correspondence teacher, who is surprised to learn that the lessons from his school actually work! <br /><br />Although the special effects may seem a little dated at first, once you get used to them they become part of the charm of this movie. In fact, the movie won an Oscar for these effects! <br /><br />The movie is innocent and fun - and it\\'s hard not to like the tuneful songs. The characters are wonderfully interesting to watch. I think anyone at any age could find something to like about this movie.'\n",
      "0 b\"As big as a Texas prairie and equally as boring. Even Liz Taylor, James Dean, Chill Wills, and Dennis Hopper can't float this overbloated boat. Taylor actually LOOKS bad--wrong wardrobe, wrong hair, and wrong makeup--a unique accomplishment in her remarkable career. Hopper gives the only believable performance, and Dean in the climactic scene displays remarkable talent as something we usually don't remember him for--a comic actor. Rock Hudson is his usual prototype of Barbie Doll Ken and makes one wonder what a, say, Redford could have done with the male lead. There is no discernible plot that provides any tension until the final twenty minutes, just a pastiche of milestones that have little relationship to each other. Except for Hopper, there is no character development, only a collection of cardboard cutouts that pop up periodically for no discernible reason like random targets in a shooting gallery. To its credit, the film does tackle racism and sexism at a time when they were taboo subjects, and it does have SIZE, making it an excellent choice for ridding yourself of unwelcome house guests. Those with the DVD version can spare themselves some of the tedium by starting with the second disk. You won't be missing anything of interest.\"\n",
      "1 b'I see a lot of folks on this site wishing AG would come out on DVD. Well, I bought it on DVD. (From Borders, no less!) While it is great to have this terrific show in a boxed DVD form, I am upset by the fact that they added very few in the way of \"extra\\'s\" (A director commentary from Shaun Cassidy on the Pilot episode) and the episodes are shown in the same order they were put out on TV. The missing episodes that were never shown prior to being run on Sci Fi channel are in the box set, but are tacked on the final DVD. If you buy the DVD set, get the actual order they are to be viewed and you will be happier. (You will need to swap DVD\\'s in and out of your player to see them in order, but you will be glad you did.)<br /><br />S'\n",
      "1 b'Your average garden variety psychotic nutcase (deliciously essayed with unhinged glee by Stephen Sachs) knocks off various dim-witted young \"adults\" (to use the term very loosely) in Dayton Hall University, which is being closed down for demolition. Featuring dreadful acting by the entire cast (Daphne Zuniga makes her ignominious and inauspicious film debut here as Debbie, a bimbo who has her head crushed by a car!), a hefty corpse tally of 10, okay make-up f/x by Matthew Mungle, a few bloody murders (baseball bat bludgeoning, chicken wire strangulation, your standard drill through the head bit, that sort of gruesome thing), a downbeat surprise twist ending which was later copied in \"Intruder,\" a creepy score by Christopher (\"Hellraiser\") Young, a slight smidgen of gratuitous female nudity, and endearingly incompetent direction by Jeffrey Obrow and Steve Carpenter (who also blessed us with \"The Power\" and \"The Kindred\"), this entertainingly abysmal slice\\'n\\'dice atrocity sizes up as a good deal of delectably dopey and drecky low-grade fun.'\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(5):\n",
    "    print(label_batch[i].numpy(), text_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed a 1,000 word vocabulary into 5 dimensions.\n",
    "embedding_layer = tf.keras.layers.Embedding(1000, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00640338,  0.02891463,  0.03818734,  0.01890494,  0.00337709],\n",
       "       [ 0.04226183,  0.01441587,  0.03041745, -0.03848713,  0.03746002],\n",
       "       [ 0.01426921,  0.03423483, -0.01242656, -0.01156135, -0.00521504]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([1, 2, 3]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([[0, 1, 2], [3, 4, 5]]))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "17/17 [==============================] - 3s 126ms/step - loss: 0.6874 - accuracy: 0.5803 - val_loss: 0.6842 - val_accuracy: 0.5634\n",
      "Epoch 2/15\n",
      "17/17 [==============================] - 2s 91ms/step - loss: 0.6775 - accuracy: 0.5803 - val_loss: 0.6783 - val_accuracy: 0.5634\n",
      "Epoch 3/15\n",
      "17/17 [==============================] - 2s 93ms/step - loss: 0.6698 - accuracy: 0.5803 - val_loss: 0.6715 - val_accuracy: 0.5634\n",
      "Epoch 4/15\n",
      "17/17 [==============================] - 2s 90ms/step - loss: 0.6596 - accuracy: 0.5803 - val_loss: 0.6596 - val_accuracy: 0.5634\n",
      "Epoch 5/15\n",
      "17/17 [==============================] - 2s 93ms/step - loss: 0.6456 - accuracy: 0.5803 - val_loss: 0.6444 - val_accuracy: 0.5634\n",
      "Epoch 6/15\n",
      "17/17 [==============================] - 2s 96ms/step - loss: 0.6273 - accuracy: 0.5803 - val_loss: 0.6259 - val_accuracy: 0.5634\n",
      "Epoch 7/15\n",
      "17/17 [==============================] - 2s 95ms/step - loss: 0.6050 - accuracy: 0.5803 - val_loss: 0.6043 - val_accuracy: 0.5634\n",
      "Epoch 8/15\n",
      "17/17 [==============================] - 2s 91ms/step - loss: 0.5796 - accuracy: 0.5803 - val_loss: 0.5806 - val_accuracy: 0.5634\n",
      "Epoch 9/15\n",
      "17/17 [==============================] - 2s 91ms/step - loss: 0.5517 - accuracy: 0.5806 - val_loss: 0.5555 - val_accuracy: 0.5677\n",
      "Epoch 10/15\n",
      "17/17 [==============================] - 2s 91ms/step - loss: 0.5223 - accuracy: 0.6182 - val_loss: 0.5300 - val_accuracy: 0.6444\n",
      "Epoch 11/15\n",
      "17/17 [==============================] - 2s 96ms/step - loss: 0.4920 - accuracy: 0.6986 - val_loss: 0.5047 - val_accuracy: 0.6769\n",
      "Epoch 12/15\n",
      "17/17 [==============================] - 2s 93ms/step - loss: 0.4622 - accuracy: 0.7414 - val_loss: 0.4814 - val_accuracy: 0.7081\n",
      "Epoch 13/15\n",
      "17/17 [==============================] - 2s 91ms/step - loss: 0.4342 - accuracy: 0.7741 - val_loss: 0.4605 - val_accuracy: 0.7378\n",
      "Epoch 14/15\n",
      "17/17 [==============================] - 2s 91ms/step - loss: 0.4084 - accuracy: 0.7978 - val_loss: 0.4426 - val_accuracy: 0.7621\n",
      "Epoch 15/15\n",
      "17/17 [==============================] - 2s 98ms/step - loss: 0.3853 - accuracy: 0.8160 - val_loss: 0.4274 - val_accuracy: 0.7741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0a2020bb50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, 100)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 5509), started 0:00:14 ago. (Use '!kill 5509' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-faf7412e93e7301e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-faf7412e93e7301e\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "  files.download('vectors.tsv')\n",
    "  files.download('metadata.tsv')\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('neural_n')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a0e8d9c06cab710e9ae9691e7d0417c31968365137cec4f6d7aac84a7634fa2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
